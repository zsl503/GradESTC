# Full explaination are listed on README.md

mode: serial # [serial, parallel]

parallel: # It's fine to keep these configs.
  # Go check doc of `https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html` for more details.
  ray_cluster_addr: null # [null, auto, local]

  # `null` implies that all cpus/gpus are included.
  num_cpus: null
  num_gpus: null

  # should be set larger than 1, or training mode fallback to `serial`
  # Set a larger `num_workers` can further boost efficiency, also let each worker have less computational resources.
  num_workers: 10

common:
  # [mnist, cifar10, cifar100, emnist, fmnist, femnist, medmnist, medmnistA, medmnistC, covid19, celeba, synthetic, svhn, tiny_imagenet, cinic10, domain]
  output_dir: "iid_10"
  data_dir: "iid_10"
  dataset: cifar10
  seed: 42
  model: res18
  join_ratio: 1
  global_epoch: 200
  local_epoch: 5
  finetune_epoch: 5
  batch_size: 32
  test_interval: 500
  straggler_ratio: 0
  straggler_min_local_epoch: 0
  external_model_params_file: ""
  buffers: local # [local, global, drop]
  reset_optimizer: false
  optimizer:
    name: adam # [sgd, adam, adamw, rmsprop, adagrad]
    lr: 0.03
    dampening: 0 # for SGD
    weight_decay: 0
    momentum: 0 # for [SGD, RMSprop]
    alpha: 0.99 # for RMSprop
    nesterov: false # for SGD
    betas: [0.9, 0.999] # for [Adam, AdamW]
    amsgrad: false # for [Adam, AdamW]

  desc: "resnet18"

  lr_scheduler:
    name: null # [null, step, cosine, constant, plateau]
    step_size: 10 # an arg example for setting step lr_scheduler

  eval_test: true
  eval_val: false
  eval_train: false

  verbose_gap: 1
  visible: tensorboard # [null, visdom, tensorboard]
  use_cuda: true
  save_log: true
  save_model: true
  save_fig: true
  save_metrics: true
  check_convergence: true

fedtopk:
  topk: 10
  sparse_format: csr # [csr, coo]
  # Only the parameters in compress_layer will be compressed. If it is None, all parameters will be compressed. If it is [], no parameters will be compressed.
  compress_layer:

fedpaq:
  compress_level: 8
  compress_global: true
  # compress_layer: ["base.layer3.0.conv1.weight", "base.layer3.0.conv2.weight", "base.layer3.1.conv1.weight", "base.layer3.1.conv2.weight", "base.layer4.0.conv1.weight", "base.layer4.0.conv2.weight", "base.layer4.1.conv1.weight", "base.layer4.1.conv2.weight"]
  compress_layer:

gradestc:
  u_type: float32
  upload_setting_dict:
    base.layer3.0.conv1.weight: [32, 32, 1152]  # 64, 64, 128*3*3 | shape: torch.Size([256, 128, 3, 3])
    base.layer3.0.conv2.weight: [32, 32, 2304]  # 64, 64, 256*3*3 | shape: torch.Size([256, 256, 3, 3])

    base.layer3.1.conv1.weight: [32, 32, 768]  # 64, 64, 256*3 | shape: torch.Size([256, 256, 3, 3])
    base.layer3.1.conv2.weight: [32, 32, 768]  # 64, 64, 256*3 | shape: torch.Size([256, 256, 3, 3])
    base.layer4.0.conv1.weight: [32, 32, 1024]  # 64, 64, 256*3 | shape: torch.Size([512, 256, 3, 3])
    base.layer4.0.conv2.weight: [32, 32, 1536]  # 64, 64, 256*3 | shape: torch.Size([512, 512, 3, 3])
    base.layer4.1.conv1.weight: [32, 32, 1536]  # 64, 64, 256*3 | shape: torch.Size([512, 512, 3, 3])
    base.layer4.1.conv2.weight: [32, 32, 1536]  # 64, 64, 256*3 | shape: torch.Size([512, 512, 3, 3])

  broadcast_setting_dict:
    # base.layer3.0.conv1.weight: [32, 32, 1152]  # 64, 64, 128*3*3 | shape: torch.Size([256, 128, 3, 3])
    # base.layer3.0.conv2.weight: [32, 32, 2304]  # 64, 64, 256*3*3 | shape: torch.Size([256, 256, 3, 3])

    # base.layer3.1.conv1.weight: [32, 32, 768]  # 64, 64, 256*3 | shape: torch.Size([256, 256, 3, 3])
    # base.layer3.1.conv2.weight: [32, 32, 768]  # 64, 64, 256*3 | shape: torch.Size([256, 256, 3, 3])
    # base.layer4.0.conv1.weight: [32, 32, 1024]  # 64, 64, 256*3 | shape: torch.Size([512, 256, 3, 3])
    # base.layer4.0.conv2.weight: [32, 32, 1536]  # 64, 64, 256*3 | shape: torch.Size([512, 512, 3, 3])
    # base.layer4.1.conv1.weight: [32, 32, 1536]  # 64, 64, 256*3 | shape: torch.Size([512, 512, 3, 3])
    # base.layer4.1.conv2.weight: [32, 32, 1536]  # 64, 64, 256*3 | shape: torch.Size([512, 512, 3, 3])


fedssd:
  u_type: float32
  upload_setting_dict:
    base.layer3.0.conv1.weight: [32, 32, 1152]  # 64, 64, 128*3*3 | shape: torch.Size([256, 128, 3, 3])
    base.layer3.0.conv2.weight: [32, 32, 2304]  # 64, 64, 256*3*3 | shape: torch.Size([256, 256, 3, 3])

    base.layer3.1.conv1.weight: [32, 32, 768]  # 64, 64, 256*3 | shape: torch.Size([256, 256, 3, 3])
    base.layer3.1.conv2.weight: [32, 32, 768]  # 64, 64, 256*3 | shape: torch.Size([256, 256, 3, 3])
    base.layer4.0.conv1.weight: [32, 32, 1024]  # 64, 64, 256*3 | shape: torch.Size([512, 256, 3, 3])
    base.layer4.0.conv2.weight: [32, 32, 1536]  # 64, 64, 256*3 | shape: torch.Size([512, 512, 3, 3])
    base.layer4.1.conv1.weight: [32, 32, 1536]  # 64, 64, 256*3 | shape: torch.Size([512, 512, 3, 3])
    base.layer4.1.conv2.weight: [32, 32, 1536]  # 64, 64, 256*3 | shape: torch.Size([512, 512, 3, 3])


svdfed:
  kp: 1
  ki: 1
  kd: 1
  gamma: 6
  fixed_adj_freq: 0
  setting_dict:
    base.layer3.0.conv1.weight: [3, 0.2]  # L, R
    base.layer3.0.conv2.weight: [3, 0.2]  # L, R

    base.layer3.1.conv1.weight: [3, 0.2]  # L, R
    base.layer3.1.conv2.weight: [3, 0.2]  # L, R
    base.layer4.0.conv1.weight: [3, 0.2]  # L, R
    base.layer4.0.conv2.weight: [3, 0.2]  # L, R
    base.layer4.1.conv1.weight: [3, 0.2]  # L, R
    base.layer4.1.conv2.weight: [3, 0.2]  # L, R
